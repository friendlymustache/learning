6

Chapter 1 The Role of Algorithms in Computing

Because many programs use it as an intermediate step, sorting is a fundamental
operation in computer science. As a result, we have a large number of good sorting
algorithms at our disposal. Which algorithm is best for a given application depends
on—among other factors—the number of items to be sorted, the extent to which
the items are already somewhat sorted, possible restrictions on the item values,
the architecture of the computer, and the kind of storage devices to be used: main
memory, disks, or even tapes.

An algorithm is said to be correct if, for every input instance, it halts with the
correct output. We say that a correct algorithm solves the given computational
problem. An incorrect algorithm might not halt at all on some input instances, or it
might halt with an incorrect answer. Contrary to what you might expect, incorrect
algorithms can sometimes be useful, if we can control their error rate. We shall see
an example of an algorithm with a controllable error rate in Chapter 31 when we
study algorithms for ﬁnding large prime numbers. Ordinarily, however, we shall
be concerned only with correct algorithms.

An algorithm can be speciﬁed in English, as a computer program, or even as
a hardware design. The only requirement is that the speciﬁcation must provide a
precise description of the computational procedure to be followed.

What kinds of problems are solved by algorithms?

Sorting is by no means the only computational problem for which algorithms have
been developed. (You probably suspected as much when you saw the size of this
book.) Practical applications of algorithms are ubiquitous and include the follow-
ing examples:

(cid:2) The Human Genome Project has made great progress toward the goals of iden-
tifying all the 100,000 genes in human DNA, determining the sequences of the
3 billion chemical base pairs that make up human DNA, storing this informa-
tion in databases, and developing tools for data analysis. Each of these steps
requires sophisticated algorithms. Although the solutions to the various prob-
lems involved are beyond the scope of this book, many methods to solve these
biological problems use ideas from several of the chapters in this book, thereby
enabling scientists to accomplish tasks while using resources efﬁciently. The
savings are in time, both human and machine, and in money, as more informa-
tion can be extracted from laboratory techniques.

(cid:2) The Internet enables people all around the world to quickly access and retrieve
large amounts of information. With the aid of clever algorithms, sites on the
Internet are able to manage and manipulate this large volume of data. Examples
of problems that make essential use of algorithms include ﬁnding good routes
on which the data will travel (techniques for solving such problems appear in
8

Chapter 1 The Role of Algorithms in Computing

respectively. Selecting all possible subsequences of X and Y and matching
them up could take a prohibitively long time unless m and n are very small.
We shall see in Chapter 15 how to use a general technique known as dynamic
programming to solve this problem much more efﬁciently.

(cid:2) We are given a mechanical design in terms of a library of parts, where each part
may include instances of other parts, and we need to list the parts in order so
that each part appears before any part that uses it. If the design comprises n
parts, then there are nŠ possible orders, where nŠ denotes the factorial function.
Because the factorial function grows faster than even an exponential function,
we cannot feasibly generate each possible order and then verify that, within
that order, each part appears before the parts using it (unless we have only a
few parts). This problem is an instance of topological sorting, and we shall see
in Chapter 22 how to solve this problem efﬁciently.

(cid:2) We are given n points in the plane, and we wish to ﬁnd the convex hull of
these points. The convex hull is the smallest convex polygon containing the
points. Intuitively, we can think of each point as being represented by a nail
sticking out from a board. The convex hull would be represented by a tight
rubber band that surrounds all the nails. Each nail around which the rubber
band makes a turn is a vertex of the convex hull. (See Figure 33.6 on page 1029
for an example.) Any of the 2n subsets of the points might be the vertices
of the convex hull. Knowing which points are vertices of the convex hull is
not quite enough, either, since we also need to know the order in which they
appear. There are many choices, therefore, for the vertices of the convex hull.
Chapter 33 gives two good methods for ﬁnding the convex hull.

These lists are far from exhaustive (as you again have probably surmised from
this book’s heft), but exhibit two characteristics that are common to many interest-
ing algorithmic problems:

1. They have many candidate solutions, the overwhelming majority of which do
not solve the problem at hand. Finding one that does, or one that is “best,” can
present quite a challenge.

2. They have practical applications. Of the problems in the above list, ﬁnding the
shortest path provides the easiest examples. A transportation ﬁrm, such as a
trucking or railroad company, has a ﬁnancial interest in ﬁnding shortest paths
through a road or rail network because taking shorter paths results in lower
labor and fuel costs. Or a routing node on the Internet may need to ﬁnd the
shortest path through the network in order to route a message quickly. Or a
person wishing to drive from New York to Boston may want to ﬁnd driving
directions from an appropriate Web site, or she may use her GPS while driving.
1.1 Algorithms

9

Not every problem solved by algorithms has an easily identiﬁed set of candidate
solutions. For example, suppose we are given a set of numerical values represent-
ing samples of a signal, and we want to compute the discrete Fourier transform of
these samples. The discrete Fourier transform converts the time domain to the fre-
quency domain, producing a set of numerical coefﬁcients, so that we can determine
the strength of various frequencies in the sampled signal. In addition to lying at
the heart of signal processing, discrete Fourier transforms have applications in data
compression and multiplying large polynomials and integers. Chapter 30 gives
an efﬁcient algorithm, the fast Fourier transform (commonly called the FFT), for
this problem, and the chapter also sketches out the design of a hardware circuit to
compute the FFT.

Data structures

This book also contains several data structures. A data structure is a way to store
and organize data in order to facilitate access and modiﬁcations. No single data
structure works well for all purposes, and so it is important to know the strengths
and limitations of several of them.

Technique

Although you can use this book as a “cookbook” for algorithms, you may someday
encounter a problem for which you cannot readily ﬁnd a published algorithm (many
of the exercises and problems in this book, for example). This book will teach you
techniques of algorithm design and analysis so that you can develop algorithms on
your own, show that they give the correct answer, and understand their efﬁciency.
Different chapters address different aspects of algorithmic problem solving. Some
chapters address speciﬁc problems, such as ﬁnding medians and order statistics in
Chapter 9, computing minimum spanning trees in Chapter 23, and determining a
maximum ﬂow in a network in Chapter 26. Other chapters address techniques,
such as divide-and-conquer in Chapter 4, dynamic programming in Chapter 15,
and amortized analysis in Chapter 17.

Hard problems

Most of this book is about efﬁcient algorithms. Our usual measure of efﬁciency
is speed, i.e., how long an algorithm takes to produce its result. There are some
problems, however, for which no efﬁcient solution is known. Chapter 34 studies
an interesting subset of these problems, which are known as NP-complete.

Why are NP-complete problems interesting? First, although no efﬁcient algo-
rithm for an NP-complete problem has ever been found, nobody has ever proven
10

Chapter 1 The Role of Algorithms in Computing

that an efﬁcient algorithm for one cannot exist.
In other words, no one knows
whether or not efﬁcient algorithms exist for NP-complete problems. Second, the
set of NP-complete problems has the remarkable property that if an efﬁcient algo-
rithm exists for any one of them, then efﬁcient algorithms exist for all of them. This
relationship among the NP-complete problems makes the lack of efﬁcient solutions
all the more tantalizing. Third, several NP-complete problems are similar, but not
identical, to problems for which we do know of efﬁcient algorithms. Computer
scientists are intrigued by how a small change to the problem statement can cause
a big change to the efﬁciency of the best known algorithm.

You should know about NP-complete problems because some of them arise sur-
prisingly often in real applications. If you are called upon to produce an efﬁcient
algorithm for an NP-complete problem, you are likely to spend a lot of time in a
fruitless search. If you can show that the problem is NP-complete, you can instead
spend your time developing an efﬁcient algorithm that gives a good, but not the
best possible, solution.

As a concrete example, consider a delivery company with a central depot. Each
day, it loads up each delivery truck at the depot and sends it around to deliver goods
to several addresses. At the end of the day, each truck must end up back at the depot
so that it is ready to be loaded for the next day. To reduce costs, the company wants
to select an order of delivery stops that yields the lowest overall distance traveled
by each truck. This problem is the well-known “traveling-salesman problem,” and
it is NP-complete. It has no known efﬁcient algorithm. Under certain assumptions,
however, we know of efﬁcient algorithms that give an overall distance which is
not too far above the smallest possible. Chapter 35 discusses such “approximation
algorithms.”

Parallelism

For many years, we could count on processor clock speeds increasing at a steady
rate. Physical limitations present a fundamental roadblock to ever-increasing clock
speeds, however: because power density increases superlinearly with clock speed,
chips run the risk of melting once their clock speeds become high enough. In order
to perform more computations per second, therefore, chips are being designed to
contain not just one but several processing “cores.” We can liken these multicore
computers to several sequential computers on a single chip; in other words, they are
a type of “parallel computer.” In order to elicit the best performance from multicore
computers, we need to design algorithms with parallelism in mind. Chapter 27
presents a model for “multithreaded” algorithms, which take advantage of multiple
cores. This model has advantages from a theoretical standpoint, and it forms the
basis of several successful computer programs, including a championship chess
program.
1.2 Algorithms as a technology

11

Exercises

1.1-1
Give a real-world example that requires sorting or a real-world example that re-
quires computing a convex hull.

1.1-2
Other than speed, what other measures of efﬁciency might one use in a real-world
setting?

1.1-3
Select a data structure that you have seen previously, and discuss its strengths and
limitations.

1.1-4
How are the shortest-path and traveling-salesman problems given above similar?
How are they different?

1.1-5
Come up with a real-world problem in which only the best solution will do. Then
come up with one in which a solution that is “approximately” the best is good
enough.

1.2 Algorithms as a technology

Suppose computers were inﬁnitely fast and computer memory was free. Would
you have any reason to study algorithms? The answer is yes, if for no other reason
than that you would still like to demonstrate that your solution method terminates
and does so with the correct answer.

If computers were inﬁnitely fast, any correct method for solving a problem
would do. You would probably want your implementation to be within the bounds
of good software engineering practice (for example, your implementation should
be well designed and documented), but you would most often use whichever
method was the easiest to implement.

Of course, computers may be fast, but they are not inﬁnitely fast. And memory
may be inexpensive, but it is not free. Computing time is therefore a bounded
resource, and so is space in memory. You should use these resources wisely, and
algorithms that are efﬁcient in terms of time or space will help you do so.
12

Chapter 1 The Role of Algorithms in Computing

Efﬁciency

Different algorithms devised to solve the same problem often differ dramatically in
their efﬁciency. These differences can be much more signiﬁcant than differences
due to hardware and software.

As an example, in Chapter 2, we will see two algorithms for sorting. The ﬁrst,
known as insertion sort, takes time roughly equal to c1n2 to sort n items, where c1
is a constant that does not depend on n. That is, it takes time roughly proportional
to n2. The second, merge sort, takes time roughly equal to c2n lg n, where lg n
stands for log2 n and c2 is another constant that also does not depend on n. Inser-
tion sort typically has a smaller constant factor than merge sort, so that c1 < c2.
We shall see that the constant factors can have far less of an impact on the running
time than the dependence on the input size n. Let’s write insertion sort’s running
time as c1n (cid:3) n and merge sort’s running time as c2n (cid:3) lg n. Then we see that where
insertion sort has a factor of n in its running time, merge sort has a factor of lg n,
which is much smaller. (For example, when n D 1000, lg n is approximately 10,
and when n equals one million, lg n is approximately only 20.) Although insertion
sort usually runs faster than merge sort for small input sizes, once the input size n
becomes large enough, merge sort’s advantage of lg n vs. n will more than com-
pensate for the difference in constant factors. No matter how much smaller c1 is
than c2, there will always be a crossover point beyond which merge sort is faster.
For a concrete example, let us pit a faster computer (computer A) running inser-
tion sort against a slower computer (computer B) running merge sort. They each
must sort an array of 10 million numbers. (Although 10 million numbers might
seem like a lot, if the numbers are eight-byte integers, then the input occupies
about 80 megabytes, which ﬁts in the memory of even an inexpensive laptop com-
puter many times over.) Suppose that computer A executes 10 billion instructions
per second (faster than any single sequential computer at the time of this writing)
and computer B executes only 10 million instructions per second, so that com-
puter A is 1000 times faster than computer B in raw computing power. To make
the difference even more dramatic, suppose that the world’s craftiest programmer
codes insertion sort in machine language for computer A, and the resulting code
requires 2n2 instructions to sort n numbers. Suppose further that just an average
programmer implements merge sort, using a high-level language with an inefﬁcient
compiler, with the resulting code taking 50n lg n instructions. To sort 10 million
numbers, computer A takes
2 (cid:3) .107/2 instructions
1010 instructions/second

D 20,000 seconds (more than 5.5 hours) ;

while computer B takes
1.2 Algorithms as a technology

50 (cid:3) 107 lg 107 instructions
107 instructions/second

(cid:4) 1163 seconds (less than 20 minutes) :

13

By using an algorithm whose running time grows more slowly, even with a poor
compiler, computer B runs more than 17 times faster than computer A! The advan-
tage of merge sort is even more pronounced when we sort 100 million numbers:
where insertion sort takes more than 23 days, merge sort takes under four hours.
In general, as the problem size increases, so does the relative advantage of merge
sort.

Algorithms and other technologies

The example above shows that we should consider algorithms, like computer hard-
ware, as a technology. Total system performance depends on choosing efﬁcient
algorithms as much as on choosing fast hardware. Just as rapid advances are being
made in other computer technologies, they are being made in algorithms as well.

You might wonder whether algorithms are truly that important on contemporary

computers in light of other advanced technologies, such as

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

advanced computer architectures and fabrication technologies,
easy-to-use, intuitive, graphical user interfaces (GUIs),
object-oriented systems,
integrated Web technologies, and
fast networking, both wired and wireless.

The answer is yes. Although some applications do not explicitly require algorith-
mic content at the application level (such as some simple, Web-based applications),
many do. For example, consider a Web-based service that determines how to travel
from one location to another. Its implementation would rely on fast hardware, a
graphical user interface, wide-area networking, and also possibly on object ori-
entation. However, it would also require algorithms for certain operations, such
as ﬁnding routes (probably using a shortest-path algorithm), rendering maps, and
interpolating addresses.

Moreover, even an application that does not require algorithmic content at the
application level relies heavily upon algorithms. Does the application rely on fast
hardware? The hardware design used algorithms. Does the application rely on
graphical user interfaces? The design of any GUI relies on algorithms. Does the
application rely on networking? Routing in networks relies heavily on algorithms.
Was the application written in a language other than machine code? Then it was
processed by a compiler, interpreter, or assembler, all of which make extensive use
14

Chapter 1 The Role of Algorithms in Computing

of algorithms. Algorithms are at the core of most technologies used in contempo-
rary computers.

Furthermore, with the ever-increasing capacities of computers, we use them to
solve larger problems than ever before. As we saw in the above comparison be-
tween insertion sort and merge sort, it is at larger problem sizes that the differences
in efﬁciency between algorithms become particularly prominent.

Having a solid base of algorithmic knowledge and technique is one characteristic
that separates the truly skilled programmers from the novices. With modern com-
puting technology, you can accomplish some tasks without knowing much about
algorithms, but with a good background in algorithms, you can do much, much
more.

Exercises

1.2-1
Give an example of an application that requires algorithmic content at the applica-
tion level, and discuss the function of the algorithms involved.

1.2-2
Suppose we are comparing implementations of insertion sort and merge sort on the
same machine. For inputs of size n, insertion sort runs in 8n2 steps, while merge
sort runs in 64n lg n steps. For which values of n does insertion sort beat merge
sort?

1.2-3
What is the smallest value of n such that an algorithm whose running time is 100n2
runs faster than an algorithm whose running time is 2n on the same machine?

Problems

1-1 Comparison of running times
For each function f .n/ and time t in the following table, determine the largest
size n of a problem that can be solved in time t, assuming that the algorithm to
solve the problem takes f .n/ microseconds.
Notes for Chapter 1

15

1

1

second minute

1

hour

1
day

1

month

1

year

1

century

lg np

n

n

n lg n

n2

n3

2n

nŠ

Chapter notes

There are many excellent texts on the general topic of algorithms, including those
by Aho, Hopcroft, and Ullman [5, 6]; Baase and Van Gelder [28]; Brassard and
Bratley [54]; Dasgupta, Papadimitriou, and Vazirani [82]; Goodrich and Tamassia
[148]; Hofri [175]; Horowitz, Sahni, and Rajasekaran [181]; Johnsonbaugh and
Schaefer [193]; Kingston [205]; Kleinberg and Tardos [208]; Knuth [209, 210,
211]; Kozen [220]; Levitin [235]; Manber [242]; Mehlhorn [249, 250, 251]; Pur-
dom and Brown [287]; Reingold, Nievergelt, and Deo [293]; Sedgewick [306];
Sedgewick and Flajolet [307]; Skiena [318]; and Wilf [356]. Some of the more
practical aspects of algorithm design are discussed by Bentley [42, 43] and Gonnet
[145]. Surveys of the ﬁeld of algorithms can also be found in the Handbook of The-
oretical Computer Science, Volume A [342] and the CRC Algorithms and Theory of
Computation Handbook [25]. Overviews of the algorithms used in computational
biology can be found in textbooks by Gusﬁeld [156], Pevzner [275], Setubal and
Meidanis [310], and Waterman [350].
